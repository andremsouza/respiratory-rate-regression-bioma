{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Iteration Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Python environment with Anaconda.\n",
    "\n",
    "To create a conda environment from an environment.yml file, you can use the following bash command:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup .env file\n",
    "\n",
    "It is necessary to setup a .dotenv file for connection with Label Studio and Docker. The .env file must have the following keys:\n",
    "\n",
    "```bash\n",
    "# Label Studio environment variables for API\n",
    "LABEL_STUDIO_URL=foo\n",
    "LABEL_STUDIO_API_KEY=foo\n",
    "LABEL_STUDIO_CONTAINER_ID=foo\n",
    "LABEL_STUDIO_CONTAINER_DATA_DIR=fo\n",
    "LABEL_STUDIO_DOWNLOAD_DIR=fo\n",
    "LABEL_STUDIO_PROJECT_ID=foo\n",
    "```\n",
    "\n",
    "- `LABEL_STUDIO_URL`: The URL of the Label Studio instance for API communication. Example: \"localhost:8000\"\n",
    "- `LABEL_STUDIO_API_KEY`: The API key used for authentication with the Label Studio instance.\n",
    "- `LABEL_STUDIO_CONTAINER_ID`: The ID of the Docker container used by Label Studio.\n",
    "- `LABEL_STUDIO_CONTAINER_DATA_DIR`: The directory path where the container stores data. Example: \"/label-studio/data/media/upload/\"\n",
    "- `LABEL_STUDIO_DOWNLOAD_DIR`: The directory path where downloaded files are stored. Example: \"./data/lsvideos/\"\n",
    "- `LABEL_STUDIO_PROJECT_ID`: The ID of the project in Label Studio. Example: \"6\"\n",
    "\n",
    "These keys are used to configure the connection and interaction between the interpreter and the Label Studio instance for recovering and loading data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing to repository's root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /home/andrems2305/bioma-cow-breathing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import dotenv\n",
    "\n",
    "os.chdir(os.getcwd().split(\"test\")[0])\n",
    "print(f\"cwd: {os.getcwd()}\")\n",
    "dotenv.load_dotenv()\n",
    "sys.path.append(os.getenv(\"PACKAGEPATH\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrems2305\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import os\n",
    "import random\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.video import R2Plus1D_18_Weights\n",
    "from torchvision.transforms.v2 import Compose, InterpolationMode, Normalize, Resize\n",
    "from torchvision.io import write_video\n",
    "from torchvision.transforms._presets import VideoClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "from datasets import VideoDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x74f3b4411410>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .dotenv\n",
    "dotenv.load_dotenv(dotenv_path=\".env\", verbose=True, override=True)\n",
    "# Set torch precision\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "# Set constants\n",
    "# Set argument constants\n",
    "LABEL_STUDIO_URL: str = os.getenv(\"LABEL_STUDIO_URL\")\n",
    "LABEL_STUDIO_API_KEY: str = os.getenv(\"LABEL_STUDIO_API_KEY\")\n",
    "LABEL_STUDIO_CONTAINER_ID: str = os.getenv(\"LABEL_STUDIO_CONTAINER_ID\")\n",
    "LABEL_STUDIO_CONTAINER_DATA_DIR: str = os.getenv(\"LABEL_STUDIO_CONTAINER_DATA_DIR\")\n",
    "LABEL_STUDIO_DOWNLOAD_DIR: str = os.getenv(\"LABEL_STUDIO_DOWNLOAD_DIR\")\n",
    "LABEL_STUDIO_PROJECT_ID: str = os.getenv(\"LABEL_STUDIO_PROJECT_ID\")\n",
    "TARGET_FPS: float = 5.0\n",
    "SAMPLE_SIZE: int = 16\n",
    "HOP_LENGTH: int = 8\n",
    "FILTER_TASK_IDS: list | None = None\n",
    "BBOX_TRANSFORM: bool = False\n",
    "BBOX_TRANSFORM_CORNERS: bool = False\n",
    "DOWNLOAD_VIDEOS: bool = True\n",
    "DOWNLOAD_VIDEOS_OVERWRITE: bool = False\n",
    "VERBOSE: bool = True\n",
    "MODEL_DIR: str = \"models/\"\n",
    "LOG_DIR: str = \"logs/\"\n",
    "NUM_WORKERS: int = 1\n",
    "BATCH_SIZE: int = 16\n",
    "OPTIMIZER: str = \"adamw\"\n",
    "LEARNING_RATE: float = 0.001\n",
    "WEIGHT_DECAY: float = 0.01\n",
    "MAX_EPOCHS: int = 1000\n",
    "PATIENCE: int = 8\n",
    "SEED: int = 42\n",
    "MODEL_NAME: str = \"r2plus1d18\" + \"_regression\"\n",
    "PRETRAINED: bool = True\n",
    "# Set random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Change bounding box parameter values to change data loading methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBOX_TRANSFORM: bool = True # ! Adjust this variable\n",
    "BBOX_TRANSFORM_CORNERS: bool = True # ! Adjust this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "if not os.path.exists(LABEL_STUDIO_DOWNLOAD_DIR):\n",
    "    os.makedirs(LABEL_STUDIO_DOWNLOAD_DIR, exist_ok=True)\n",
    "if not os.path.exists(\n",
    "    LOG_DIR,\n",
    "):\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-14 10:54:04.155997]: Loaded datasets\n",
      "[2024-05-14 10:54:04.156238]: Created data loaders\n"
     ]
    }
   ],
   "source": [
    "# Get transforms from weights\n",
    "if BBOX_TRANSFORM:\n",
    "    # Use r2plus1d18 transforms, without center crop\n",
    "    transform = partial(\n",
    "        VideoClassification, crop_size=(112, 112), resize_size=(112, 112)\n",
    "    )()\n",
    "else:\n",
    "    # Use r2plus1d18 default transforms\n",
    "    transform = R2Plus1D_18_Weights.DEFAULT.transforms()\n",
    "# Load dataset\n",
    "dataset = VideoDataset(\n",
    "    url=LABEL_STUDIO_URL,\n",
    "    api_key=LABEL_STUDIO_API_KEY,\n",
    "    project_id=int(LABEL_STUDIO_PROJECT_ID),\n",
    "    data_dir=LABEL_STUDIO_DOWNLOAD_DIR,\n",
    "    container_id=LABEL_STUDIO_CONTAINER_ID,\n",
    "    container_data_dir=LABEL_STUDIO_CONTAINER_DATA_DIR,\n",
    "    fps=TARGET_FPS,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    filter_task_ids=FILTER_TASK_IDS,\n",
    "    bbox_transform=BBOX_TRANSFORM,\n",
    "    bbox_transform_corners=BBOX_TRANSFORM_CORNERS,\n",
    "    download_videos=DOWNLOAD_VIDEOS,\n",
    "    download_videos_overwrite=DOWNLOAD_VIDEOS_OVERWRITE,\n",
    "    classification=False,\n",
    "    prune_invalid=False,\n",
    "    transform=transform,\n",
    "    target_transform=lambda x: torch.tensor(x).unsqueeze(0),\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "# Get task ids\n",
    "task_ids = dataset.annotations[\"id\"].unique()\n",
    "# Split task ids into train and test\n",
    "train_task_ids, test_task_ids = train_test_split(\n",
    "    task_ids, test_size=0.2, random_state=SEED\n",
    ")\n",
    "# Split dataset into train and test\n",
    "# del dataset\n",
    "train_dataset = VideoDataset(\n",
    "    url=LABEL_STUDIO_URL,\n",
    "    api_key=LABEL_STUDIO_API_KEY,\n",
    "    project_id=int(LABEL_STUDIO_PROJECT_ID),\n",
    "    data_dir=LABEL_STUDIO_DOWNLOAD_DIR,\n",
    "    container_id=LABEL_STUDIO_CONTAINER_ID,\n",
    "    container_data_dir=LABEL_STUDIO_CONTAINER_DATA_DIR,\n",
    "    fps=TARGET_FPS,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    filter_task_ids=train_task_ids,\n",
    "    bbox_transform=BBOX_TRANSFORM,\n",
    "    bbox_transform_corners=BBOX_TRANSFORM_CORNERS,\n",
    "    download_videos=DOWNLOAD_VIDEOS,\n",
    "    download_videos_overwrite=DOWNLOAD_VIDEOS_OVERWRITE,\n",
    "    classification=False,\n",
    "    prune_invalid=True,\n",
    "    transform=transform,\n",
    "    target_transform=lambda x: torch.tensor(x).unsqueeze(0),\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "test_dataset = VideoDataset(\n",
    "    url=LABEL_STUDIO_URL,\n",
    "    api_key=LABEL_STUDIO_API_KEY,\n",
    "    project_id=int(LABEL_STUDIO_PROJECT_ID),\n",
    "    data_dir=LABEL_STUDIO_DOWNLOAD_DIR,\n",
    "    container_id=LABEL_STUDIO_CONTAINER_ID,\n",
    "    container_data_dir=LABEL_STUDIO_CONTAINER_DATA_DIR,\n",
    "    fps=TARGET_FPS,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    filter_task_ids=test_task_ids,\n",
    "    bbox_transform=BBOX_TRANSFORM,\n",
    "    bbox_transform_corners=BBOX_TRANSFORM_CORNERS,\n",
    "    download_videos=DOWNLOAD_VIDEOS,\n",
    "    download_videos_overwrite=DOWNLOAD_VIDEOS_OVERWRITE,\n",
    "    classification=False,\n",
    "    prune_invalid=True,\n",
    "    transform=transform,\n",
    "    target_transform=lambda x: torch.tensor(x).unsqueeze(0),\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "print(f\"[{datetime.now()}]: Loaded datasets\")\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(f\"[{datetime.now()}]: Created data loaders\")\n",
    "# Create model\n",
    "# Set dataloaders (for generic use)\n",
    "train_dataloaders: list[DataLoader] = [train_dataloader]\n",
    "test_dataloaders: list[DataLoader] = [test_dataloader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset iteration methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sample from the dataset and save it to .mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-14 10:54:05.174049]: Got sample\n",
      "[2024-05-14 10:54:05.488286]: Saved sample as video sample.mp4\n"
     ]
    }
   ],
   "source": [
    "# Get a sample from the dataset and save it\n",
    "train_dataset.transform = None\n",
    "sample, _ = train_dataset[0]\n",
    "print(f\"[{datetime.now()}]: Got sample\")\n",
    "# Save sample as video\n",
    "write_video(\n",
    "    os.path.join(\"sample.mp4\"),\n",
    "    # sample.permute(1, 2, 3, 0),\n",
    "    sample.permute(0, 2, 3, 1),\n",
    "    fps=TARGET_FPS,\n",
    ")\n",
    "print(f\"[{datetime.now()}]: Saved sample as video {os.path.join('sample.mp4')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through dataset and save samples to output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 379/15469 [04:49<3:12:20,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "output_folder: str = \"data/output\"\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "for idx, (sample, target) in enumerate(tqdm(dataset)):\n",
    "    task_id = dataset.samples.iloc[idx].task_id\n",
    "    segment_id = dataset.samples.iloc[idx].segment_id\n",
    "    sample_id = dataset.samples.iloc[idx].sample_id\n",
    "    write_video(\n",
    "        os.path.join(output_folder, f\"{task_id}_{segment_id}_{sample_id}.mp4\"),\n",
    "        # [C, T, H, W] -> [T, H, W, C]\n",
    "        sample.permute(1, 2, 3, 0),\n",
    "        # sample.permute(0, 2, 3, 1),\n",
    "        fps=TARGET_FPS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through entire dataloaders to test functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-14 11:14:53.193357]: Got dataloaders\n",
      "[2024-05-14 11:14:53.193405]: Iterating through train dataloader\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate through samples in dataloader\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Iterating through train dataloader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m]: Got batch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Print shapes\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "# Iterate through train and test datasets with dataloaders\n",
    "break_early: bool = True\n",
    "for train_dataloader, test_dataloader in zip(train_dataloaders, test_dataloaders):\n",
    "    print(f\"[{datetime.now()}]: Got dataloaders\")\n",
    "    # Iterate through samples in dataloader\n",
    "    print(f\"[{datetime.now()}]: Iterating through train dataloader\")\n",
    "    for i, (x, y) in enumerate(train_dataloader):\n",
    "        print(f\"[{datetime.now()}]: Got batch {i}\")\n",
    "        # Print shapes\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        print(f\"y.shape: {y.shape}\")\n",
    "        if break_early:\n",
    "            break\n",
    "    print(f\"[{datetime.now()}]: Finished iterating through train dataloader\")\n",
    "    print(f\"[{datetime.now()}]: Iterating through test dataloader\")\n",
    "    for i, (x, y) in enumerate(test_dataloader):\n",
    "        print(f\"[{datetime.now()}]: Got batch {i}\")\n",
    "        # Print shapes\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        print(f\"y.shape: {y.shape}\")\n",
    "        if break_early:\n",
    "            break\n",
    "    print(f\"[{datetime.now()}]: Finished iterating through test dataloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
