{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Iteration Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Python environment with Anaconda.\n",
    "\n",
    "To create a conda environment from an environment.yml file, you can use the following bash command:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup .env file\n",
    "\n",
    "It is necessary to setup a .dotenv file for connection with Label Studio and Docker. The .env file must have the following keys:\n",
    "\n",
    "```bash\n",
    "# Label Studio environment variables for API\n",
    "LABEL_STUDIO_URL=foo\n",
    "LABEL_STUDIO_API_KEY=foo\n",
    "LABEL_STUDIO_CONTAINER_ID=foo\n",
    "LABEL_STUDIO_CONTAINER_DATA_DIR=foo\n",
    "LABEL_STUDIO_DOWNLOAD_DIR=foo\n",
    "LABEL_STUDIO_PROJECT_ID=foo\n",
    "```\n",
    "\n",
    "- `LABEL_STUDIO_URL`: The URL of the Label Studio instance for API communication. Example: \"localhost:8000\"\n",
    "- `LABEL_STUDIO_API_KEY`: The API key used for authentication with the Label Studio instance.\n",
    "- `LABEL_STUDIO_CONTAINER_ID`: The ID of the Docker container used by Label Studio.\n",
    "- `LABEL_STUDIO_CONTAINER_DATA_DIR`: The directory path where the container stores data. Example: \"/label-studio/data/media/upload/\"\n",
    "- `LABEL_STUDIO_DOWNLOAD_DIR`: The directory path where downloaded files are stored. Example: \"./data/lsvideos/\"\n",
    "- `LABEL_STUDIO_PROJECT_ID`: The ID of the project in Label Studio. Example: \"6\"\n",
    "\n",
    "These keys are used to configure the connection and interaction between the interpreter and the Label Studio instance for recovering and loading data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing to repository's root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /home/andrems2305/bioma-cow-breathing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import dotenv\n",
    "\n",
    "os.chdir(os.getcwd().split(\"test\")[0])\n",
    "print(f\"cwd: {os.getcwd()}\")\n",
    "dotenv.load_dotenv()\n",
    "sys.path.append(os.getenv(\"PACKAGEPATH\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrems2305\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrems2305/anaconda3/envs/torch/lib/python3.11/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import os\n",
    "import random\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.video import R2Plus1D_18_Weights\n",
    "from torchvision.transforms.v2 import Compose, InterpolationMode, Normalize, Resize\n",
    "from torchvision.io import write_video\n",
    "from torchvision.transforms._presets import VideoClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "from datasets import VideoDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x786859dfd410>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .dotenv\n",
    "dotenv.load_dotenv(dotenv_path=\".env\", verbose=True, override=True)\n",
    "# Set torch precision\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "# Set constants\n",
    "# Set argument constants\n",
    "LABEL_STUDIO_URL: str = os.getenv(\"LABEL_STUDIO_URL\")\n",
    "LABEL_STUDIO_API_KEY: str = os.getenv(\"LABEL_STUDIO_API_KEY\")\n",
    "LABEL_STUDIO_CONTAINER_ID: str = os.getenv(\"LABEL_STUDIO_CONTAINER_ID\")\n",
    "LABEL_STUDIO_CONTAINER_DATA_DIR: str = os.getenv(\"LABEL_STUDIO_CONTAINER_DATA_DIR\")\n",
    "LABEL_STUDIO_DOWNLOAD_DIR: str = os.getenv(\"LABEL_STUDIO_DOWNLOAD_DIR\")\n",
    "LABEL_STUDIO_PROJECT_ID: str = os.getenv(\"LABEL_STUDIO_PROJECT_ID\")\n",
    "TARGET_FPS: float = 5.0\n",
    "SAMPLE_SIZE: int = 16\n",
    "HOP_LENGTH: int = 8\n",
    "FILTER_TASK_IDS: list | None = None\n",
    "BBOX_TRANSFORM: bool = False\n",
    "BBOX_TRANSFORM_CORNERS: bool = False\n",
    "DOWNLOAD_VIDEOS: bool = True\n",
    "DOWNLOAD_VIDEOS_OVERWRITE: bool = False\n",
    "VERBOSE: bool = True\n",
    "MODEL_DIR: str = \"models/\"\n",
    "LOG_DIR: str = \"logs/\"\n",
    "NUM_WORKERS: int = 1\n",
    "BATCH_SIZE: int = 16\n",
    "OPTIMIZER: str = \"adamw\"\n",
    "LEARNING_RATE: float = 0.001\n",
    "WEIGHT_DECAY: float = 0.01\n",
    "MAX_EPOCHS: int = 1000\n",
    "PATIENCE: int = 8\n",
    "SEED: int = 42\n",
    "MODEL_NAME: str = \"r2plus1d18\" + \"_regression\"\n",
    "PRETRAINED: bool = True\n",
    "# Set random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Change bounding box parameter values to change data loading methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBOX_TRANSFORM: bool = True # ! Adjust this variable\n",
    "BBOX_TRANSFORM_CORNERS: bool = True # ! Adjust this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "if not os.path.exists(LABEL_STUDIO_DOWNLOAD_DIR):\n",
    "    os.makedirs(LABEL_STUDIO_DOWNLOAD_DIR, exist_ok=True)\n",
    "if not os.path.exists(\n",
    "    LOG_DIR,\n",
    "):\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-14 16:18:14.857009]: Loading datasets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-14 16:20:28.729073]: Loaded full dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m\n\u001b[1;32m     37\u001b[0m train_task_ids, test_task_ids \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     38\u001b[0m     task_ids, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mSEED\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Split dataset into train and test\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# del dataset\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mVideoDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLABEL_STUDIO_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLABEL_STUDIO_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLABEL_STUDIO_PROJECT_ID\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLABEL_STUDIO_DOWNLOAD_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLABEL_STUDIO_CONTAINER_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_data_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLABEL_STUDIO_CONTAINER_DATA_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_FPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLE_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHOP_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_task_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_task_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBBOX_TRANSFORM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox_transform_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBBOX_TRANSFORM_CORNERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_videos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDOWNLOAD_VIDEOS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_videos_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDOWNLOAD_VIDEOS_OVERWRITE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprune_invalid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVERBOSE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Loaded train dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m VideoDataset(\n\u001b[1;32m     65\u001b[0m     url\u001b[38;5;241m=\u001b[39mLABEL_STUDIO_URL,\n\u001b[1;32m     66\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mLABEL_STUDIO_API_KEY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mVERBOSE,\n\u001b[1;32m     84\u001b[0m )\n",
      "File \u001b[0;32m~/bioma-cow-breathing/datasets.py:161\u001b[0m, in \u001b[0;36mVideoDataset.__init__\u001b[0;34m(self, url, api_key, project_id, data_dir, container_id, container_data_dir, fps, sample_size, hop_length, filter_task_ids, bbox_transform, bbox_transform_corners, download_videos, download_videos_overwrite, classification, prune_invalid, transform, target_transform, verbose)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_annotations()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Process annotations\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Prune invalid samples\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_invalid:\n",
      "File \u001b[0;32m~/bioma-cow-breathing/datasets.py:325\u001b[0m, in \u001b[0;36mVideoDataset._process_annotations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegments \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(segments)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Convert segments to samples\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m sample_lists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_segment_to_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Merge sample lists into single list\u001b[39;00m\n\u001b[1;32m    330\u001b[0m samples \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/bioma-cow-breathing/datasets.py:424\u001b[0m, in \u001b[0;36mVideoDataset._segment_to_samples\u001b[0;34m(self, segment)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# Read video timestamps\u001b[39;00m\n\u001b[1;32m    423\u001b[0m filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir, segment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 424\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m video_fps: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FPS)\n\u001b[1;32m    426\u001b[0m frame_count: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_COUNT))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get transforms from weights\n",
    "if BBOX_TRANSFORM:\n",
    "    # Use r2plus1d18 transforms, without center crop\n",
    "    transform = partial(\n",
    "        VideoClassification, crop_size=(112, 112), resize_size=(112, 112)\n",
    "    )()\n",
    "else:\n",
    "    # Use r2plus1d18 default transforms\n",
    "    transform = R2Plus1D_18_Weights.DEFAULT.transforms()\n",
    "print(f\"[{datetime.now()}]: Loading datasets\")\n",
    "# Load dataset\n",
    "dataset = VideoDataset(\n",
    "    url=LABEL_STUDIO_URL,\n",
    "    api_key=LABEL_STUDIO_API_KEY,\n",
    "    project_id=int(LABEL_STUDIO_PROJECT_ID),\n",
    "    data_dir=LABEL_STUDIO_DOWNLOAD_DIR,\n",
    "    container_id=LABEL_STUDIO_CONTAINER_ID,\n",
    "    container_data_dir=LABEL_STUDIO_CONTAINER_DATA_DIR,\n",
    "    fps=TARGET_FPS,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    filter_task_ids=FILTER_TASK_IDS,\n",
    "    bbox_transform=BBOX_TRANSFORM,\n",
    "    bbox_transform_corners=BBOX_TRANSFORM_CORNERS,\n",
    "    download_videos=DOWNLOAD_VIDEOS,\n",
    "    download_videos_overwrite=DOWNLOAD_VIDEOS_OVERWRITE,\n",
    "    classification=False,\n",
    "    prune_invalid=False,\n",
    "    transform=transform,\n",
    "    target_transform=lambda x: torch.tensor(x).unsqueeze(0),\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "print(f\"[{datetime.now()}]: Loaded full dataset\")\n",
    "# Get task ids\n",
    "task_ids = dataset.annotations[\"id\"].unique()\n",
    "# Split task ids into train and test\n",
    "train_task_ids, test_task_ids = train_test_split(\n",
    "    task_ids, test_size=0.2, random_state=SEED\n",
    ")\n",
    "# Split dataset into train and test\n",
    "# del dataset\n",
    "train_dataset = VideoDataset(\n",
    "    url=LABEL_STUDIO_URL,\n",
    "    api_key=LABEL_STUDIO_API_KEY,\n",
    "    project_id=int(LABEL_STUDIO_PROJECT_ID),\n",
    "    data_dir=LABEL_STUDIO_DOWNLOAD_DIR,\n",
    "    container_id=LABEL_STUDIO_CONTAINER_ID,\n",
    "    container_data_dir=LABEL_STUDIO_CONTAINER_DATA_DIR,\n",
    "    fps=TARGET_FPS,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    filter_task_ids=train_task_ids,\n",
    "    bbox_transform=BBOX_TRANSFORM,\n",
    "    bbox_transform_corners=BBOX_TRANSFORM_CORNERS,\n",
    "    download_videos=DOWNLOAD_VIDEOS,\n",
    "    download_videos_overwrite=DOWNLOAD_VIDEOS_OVERWRITE,\n",
    "    classification=False,\n",
    "    prune_invalid=True,\n",
    "    transform=transform,\n",
    "    target_transform=lambda x: torch.tensor(x).unsqueeze(0),\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "print(f\"[{datetime.now()}]: Loaded train dataset\")\n",
    "test_dataset = VideoDataset(\n",
    "    url=LABEL_STUDIO_URL,\n",
    "    api_key=LABEL_STUDIO_API_KEY,\n",
    "    project_id=int(LABEL_STUDIO_PROJECT_ID),\n",
    "    data_dir=LABEL_STUDIO_DOWNLOAD_DIR,\n",
    "    container_id=LABEL_STUDIO_CONTAINER_ID,\n",
    "    container_data_dir=LABEL_STUDIO_CONTAINER_DATA_DIR,\n",
    "    fps=TARGET_FPS,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    filter_task_ids=test_task_ids,\n",
    "    bbox_transform=BBOX_TRANSFORM,\n",
    "    bbox_transform_corners=BBOX_TRANSFORM_CORNERS,\n",
    "    download_videos=DOWNLOAD_VIDEOS,\n",
    "    download_videos_overwrite=DOWNLOAD_VIDEOS_OVERWRITE,\n",
    "    classification=False,\n",
    "    prune_invalid=True,\n",
    "    transform=transform,\n",
    "    target_transform=lambda x: torch.tensor(x).unsqueeze(0),\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "print(f\"[{datetime.now()}]: Loaded test dataset\")\n",
    "print(f\"[{datetime.now()}]: Loaded datasets\")\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(f\"[{datetime.now()}]: Created data loaders\")\n",
    "# Create model\n",
    "# Set dataloaders (for generic use)\n",
    "train_dataloaders: list[DataLoader] = [train_dataloader]\n",
    "test_dataloaders: list[DataLoader] = [test_dataloader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset iteration methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sample from the dataset and save it to .mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-14 16:21:32.506245]: Got sample\n",
      "[2024-05-14 16:21:32.872849]: Saved sample as video sample.mp4\n"
     ]
    }
   ],
   "source": [
    "# Get a sample from the dataset and save it\n",
    "dataset.transform = None # ! Uncomment if you want a high-resolution video\n",
    "sample, _ = dataset[0]\n",
    "print(f\"[{datetime.now()}]: Got sample\")\n",
    "# Save sample as video\n",
    "write_video(\n",
    "    os.path.join(\"sample.mp4\"),\n",
    "    # sample.permute(1, 2, 3, 0), # ! Comment this line if dataset.transform == None\n",
    "    sample.permute(0, 2, 3, 1), # ! Uncomment if dataset.transform == None\n",
    "    fps=TARGET_FPS,\n",
    ")\n",
    "print(f\"[{datetime.now()}]: Saved sample as video {os.path.join('sample.mp4')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through dataset and save samples to output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 77/15469 [01:35<5:19:43,  1.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m segment_id \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39msamples\u001b[38;5;241m.\u001b[39miloc[idx]\u001b[38;5;241m.\u001b[39msegment_id\n\u001b[1;32m      8\u001b[0m sample_id \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39msamples\u001b[38;5;241m.\u001b[39miloc[idx]\u001b[38;5;241m.\u001b[39msample_id\n\u001b[0;32m----> 9\u001b[0m \u001b[43mwrite_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtask_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msegment_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msample_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [C, T, H, W] -> [T, H, W, C]\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# sample.permute(1, 2, 3, 0), # ! Comment this line if dataset.transform == None\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# ! Uncomment if dataset.transform == None\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_FPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/io/video.py:133\u001b[0m, in \u001b[0;36mwrite_video\u001b[0;34m(filename, video_array, fps, video_codec, options, audio_array, audio_fps, audio_codec, audio_options)\u001b[0m\n\u001b[1;32m    131\u001b[0m     frame \u001b[38;5;241m=\u001b[39m av\u001b[38;5;241m.\u001b[39mVideoFrame\u001b[38;5;241m.\u001b[39mfrom_ndarray(img, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb24\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m     frame\u001b[38;5;241m.\u001b[39mpict_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNONE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m packet \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    134\u001b[0m         container\u001b[38;5;241m.\u001b[39mmux(packet)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Flush stream\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_folder: str = \"data/output\"\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "for idx, (sample, target) in enumerate(tqdm(dataset)):\n",
    "    task_id = dataset.samples.iloc[idx].task_id\n",
    "    segment_id = dataset.samples.iloc[idx].segment_id\n",
    "    sample_id = dataset.samples.iloc[idx].sample_id\n",
    "    write_video(\n",
    "        os.path.join(output_folder, f\"{task_id}_{segment_id}_{sample_id}.mp4\"),\n",
    "        # [C, T, H, W] -> [T, H, W, C]\n",
    "        # sample.permute(1, 2, 3, 0), # ! Comment this line if dataset.transform == None\n",
    "        sample.permute(0, 2, 3, 1), # ! Uncomment if dataset.transform == None\n",
    "        fps=TARGET_FPS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through entire dataloaders to test functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through train and test datasets with dataloaders\n",
    "break_early: bool = True\n",
    "for train_dataloader, test_dataloader in zip(train_dataloaders, test_dataloaders):\n",
    "    print(f\"[{datetime.now()}]: Got dataloaders\")\n",
    "    # Iterate through samples in dataloader\n",
    "    print(f\"[{datetime.now()}]: Iterating through train dataloader\")\n",
    "    for i, (x, y) in enumerate(train_dataloader):\n",
    "        print(f\"[{datetime.now()}]: Got batch {i}\")\n",
    "        # Print shapes\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        print(f\"y.shape: {y.shape}\")\n",
    "        if break_early:\n",
    "            break\n",
    "    print(f\"[{datetime.now()}]: Finished iterating through train dataloader\")\n",
    "    print(f\"[{datetime.now()}]: Iterating through test dataloader\")\n",
    "    for i, (x, y) in enumerate(test_dataloader):\n",
    "        print(f\"[{datetime.now()}]: Got batch {i}\")\n",
    "        # Print shapes\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        print(f\"y.shape: {y.shape}\")\n",
    "        if break_early:\n",
    "            break\n",
    "    print(f\"[{datetime.now()}]: Finished iterating through test dataloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
